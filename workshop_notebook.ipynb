{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24945bac",
   "metadata": {},
   "source": [
    "# Multi-Instance Vehicle Damage Detection Workshop\n",
    "# \n",
    "# [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/harpreetsahota204/odsc_west_workshop/blob/main/workshop_notebook.ipynb)\n",
    "\n",
    "\n",
    "## Act 1: Explore & Visualize \n",
    "\n",
    "### Goal\n",
    "\n",
    "Understand what makes multi-instance vehicle damage detection challenging through visual exploration and embedding analysis.\n",
    "\n",
    "> **Here's what makes this dataset realistic and challenging:**\n",
    "> \n",
    "> **First**, each image can contain MULTIPLE damages of DIFFERENT types. A single accident might cause a dent on the door, scratches on the bumper, AND a cracked taillight - all in one photo. This isn't a simple 'one object per image' scenario.\n",
    ">\n",
    "> **Second**, cars are photographed from various perspectives - front, side, rear, close-ups, wide shots. A scratch viewed straight-on looks different than the same scratch at an angle.\n",
    ">\n",
    "> **Third**, this is instance segmentation - we need to detect and segment EACH individual damage, not just classify the whole image. Much harder!\n",
    ">\n",
    "> Before we rush to train a model, let's actually understand what we're working with. This data-centric approach will save us time and improve our results.\n",
    "\n",
    "Let's start by loading our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688bf00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone.utils.huggingface import load_from_hub\n",
    "dataset = load_from_hub(\n",
    "    \"harpreetsahota/CarDD\", \n",
    "    persistent=True,\n",
    "    overwrite=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e175a85b",
   "metadata": {},
   "source": [
    "Make sure you install the [Dashboard Plugin](https://docs.voxel51.com/plugins/plugins_ecosystem/dashboard.html), which can be installed by running the following in your terminal:\n",
    "\n",
    "```bash\n",
    "fiftyone plugins download \\\n",
    "    https://github.com/voxel51/fiftyone-plugins \\\n",
    "    --plugin-names @voxel51/dashboard\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c38389",
   "metadata": {},
   "source": [
    "## Profiling the dataset\n",
    "\n",
    "### Dataset size\n",
    "\n",
    "- `len(dataset)` returns the number of samples in the dataset.  \n",
    "  [Reference](https://docs.voxel51.com/user_guide/using_datasets.html#using-datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f97d48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset size: {len(dataset)} samples (images)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5508196a",
   "metadata": {},
   "source": [
    "### Total damage instances\n",
    "\n",
    "- `dataset.count(\"detections.detections\")` counts the total number of detection objects across all samples.  \n",
    "  [Reference](https://github.com/voxel51/fiftyone/blob/develop/docs/source/tutorials/pandas_comparison.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b1ea7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total damage instances: {dataset.count('detections.detections')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118f90c8",
   "metadata": {},
   "source": [
    "### Damage types\n",
    "\n",
    "- `dataset.distinct(\"detections.detections.label\")` returns all unique labels for detections.  \n",
    "  [Reference](https://github.com/voxel51/fiftyone/blob/develop/docs/source/tutorials/pandas_comparison.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a0be3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Damage types: {dataset.distinct('detections.detections.label')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0319da85",
   "metadata": {},
   "source": [
    "### Average damages per image\n",
    "\n",
    "- This computes the mean number of detections per image. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf64fb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_damages = dataset.count('detections.detections') / len(dataset)\n",
    "\n",
    "print(f\"\\nAverage damages per image: {avg_damages:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c1f509",
   "metadata": {},
   "source": [
    "Alternatively, you can use:\n",
    "\n",
    "[Reference](https://docs.voxel51.com/user_guide/using_aggregations.html#aggregating-expressions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7d41ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone import ViewField as F\n",
    "\n",
    "avg_damages = dataset.mean(F(\"detections.detections\").length())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a27eb6",
   "metadata": {},
   "source": [
    "### Distribution of number of damages per image\n",
    "\n",
    "The idiomatic FiftyOne way to count the number of detection labels in a sample is to use a [`ViewField`](https://docs.voxel51.com/api/fiftyone.core.expressions.html#fiftyone.core.expressions.ViewField) expression to access the list of labels and then use `.length()` to count them. \n",
    "\n",
    "To add the number of damages per image as a field on each sample in your dataset, you can use FiftyOne's [`set_values()`](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset.set_values). This will efficiently compute and store the count for each sample.\n",
    "\n",
    "**References:**  \n",
    "- [Transforming fields with set_field](https://docs.voxel51.com/user_guide/using_views.html#transforming-fields)\n",
    "- [Example: Counting detections](https://docs.voxel51.com/api/fiftyone.core.dataset.html)  \n",
    "- [Pandas comparison: Adding new columns](https://docs.voxel51.com/tutorials/pandas_comparison.html#Add-a-new-column/frame-from-existing-columns/fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d735b6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "from fiftyone import ViewField as F\n",
    "\n",
    "# Add a field \"num_damages\" to each sample with the count of detection labels\n",
    "\n",
    "num_damages = dataset.values(F(\"detections.detections\").length())\n",
    "\n",
    "dataset.set_values(\"num_damages\", num_damages)\n",
    "\n",
    "dataset.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35aa1110",
   "metadata": {},
   "source": [
    "Remember to call `save()` on the view to persist the changes to the dataset itself.  \n",
    "\n",
    "[Reference: Add new column/field](https://github.com/voxel51/fiftyone/blob/develop/docs/source/tutorials/pandas_comparison.ipynb)  \n",
    "\n",
    "[Reference: Transforming fields](https://docs.voxel51.com/user_guide/using_views.html#transforming-fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d11f3e9",
   "metadata": {},
   "source": [
    "### 6. Images with multiple damages\n",
    "\n",
    "- This counts images with more than one detection.  \n",
    "  [Reference](https://docs.voxel51.com/api/fiftyone.core.view.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7997d9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_damage = len(dataset.match(F(\"detections.detections\").length() > 1))\n",
    "\n",
    "print(f\"\\nImages with multiple damages: {multi_damage} ({multi_damage/len(dataset)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1f7db3",
   "metadata": {},
   "source": [
    "You can make the query for images that have **both** \"scratch\" and \"crack\" by using the `all=True` argument in `contains()`, which checks that both values are present in the list. \n",
    "\n",
    "[Reference](https://docs.voxel51.com/cheat_sheets/filtering_cheat_sheet.html#detections)\n",
    "\n",
    "For example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b6198b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many images have both scratch AND crack?\n",
    "has_scratch_and_crack = dataset.match(\n",
    "    F(\"detections.detections.label\").contains([\"scratch\", \"crack\"], all=True)\n",
    ")\n",
    "\n",
    "dataset.save_view(\"has_scratch_and_crack\", has_scratch_and_crack)\n",
    "\n",
    "print(f\"Images with BOTH scratch and crack: {len(has_scratch_and_crack)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffceca6f",
   "metadata": {},
   "source": [
    "If you want to select images where **all** detections are \"scratch\" (i.e., the set of labels is a subset of `[\"scratch\"]`), you should use the `is_subset()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fa9f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images with ONLY scratch\n",
    "only_scratch = dataset.match(\n",
    "    F(\"detections.detections.label\").is_subset([\"scratch\"])\n",
    ")\n",
    "\n",
    "dataset.save_view(\"only_scratch\", only_scratch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b31d87e",
   "metadata": {},
   "source": [
    "This will ensure that the only label present in the detections is \"scratch\" (or the list is empty). The same logic applies for \"crack\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f835a0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "only_crack = dataset.match(\n",
    "    F(\"detections.detections.label\").is_subset([\"crack\"])\n",
    ")\n",
    "\n",
    "dataset.save_view(\"only_crack\", only_crack)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c8c104",
   "metadata": {},
   "source": [
    "This approach is documented in the [FiftyOne filtering cheat sheet](https://docs.voxel51.com/cheat_sheets/filtering_cheat_sheet.html#detections), where `is_subset()` is used to match samples that only contain a specific label and no others.\n",
    "\n",
    "**Summary:**  \n",
    "- Use `is_subset([\"scratch\"])` to match images where all detections are \"scratch\" and no other labels are present.\n",
    "- Your original approach does not exclude other label types besides \"crack.\"\n",
    "\n",
    "[Reference: Filtering Cheat Sheet](https://docs.voxel51.com/cheat_sheets/filtering_cheat_sheet.html#detections)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ba1182",
   "metadata": {},
   "source": [
    "#### Add a \"complexity_score\" field: number of damages + number of unique damage types\n",
    "\n",
    "The complexity score tries to capture \"how hard is this image to process?\"\n",
    "\n",
    "- **More damages = harder** (finding 4 damages is harder than finding 1)\n",
    "\n",
    "- **More damage types = harder** (distinguishing scratch vs dent vs crack in one image is harder than just finding 3 scratches)\n",
    "\n",
    "So it adds them: `num_damages + num_unique_types`\n",
    "\n",
    "\n",
    "##### **Why This Is Useful**\n",
    "\n",
    "**1. It captures TWO kinds of difficulty:**\n",
    "\n",
    "**Quantity Difficulty**: More damages = harder to detect all of them\n",
    "\n",
    "- Finding 1 damage is easier than finding 4 damages\n",
    "\n",
    "**Diversity Difficulty**: More damage types = harder to classify correctly\n",
    "\n",
    "- Distinguishing between 3 different damage types in one image is harder than identifying 3 of the same type\n",
    "\n",
    "### **2. It helps you stratify your data for training:**\n",
    "\n",
    "```python\n",
    "# Start training on simple cases\n",
    "simple = dataset.match(F(\"complexity_score\") <= 3)  # 1-2 damages, maybe 1-2 types\n",
    "\n",
    "# Then add moderate complexity\n",
    "moderate = dataset.match((F(\"complexity_score\") > 3) & (F(\"complexity_score\") <= 5))\n",
    "\n",
    "# Finally tackle hard cases\n",
    "hard = dataset.match(F(\"complexity_score\") > 5)  # Many damages + high diversity\n",
    "```\n",
    "\n",
    "### **3. It correlates with model performance:**\n",
    "\n",
    "You'll want to understand if:\n",
    "- Low complexity → higher precision/recall\n",
    "- High complexity → lower precision/recall\n",
    "\n",
    "This tells you WHERE to focus improvement efforts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c065bd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone import ViewField as F\n",
    "\n",
    "labels_per_sample = dataset.values(\"detections.detections.label\")\n",
    "num_distinct_labels_per_sample = [len(set(labels)) if labels else 0 for labels in labels_per_sample]\n",
    "dataset.set_values(\"num_unique_labels\", num_distinct_labels_per_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d61c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_label_counts = dataset.values(\"num_unique_labels\")\n",
    "\n",
    "# Compute complexity scores for all samples\n",
    "num_damages_values = dataset.values(\"num_damages\")\n",
    "\n",
    "complexity_scores = [nd + nul for nd, nul in zip(num_damages_values, unique_label_counts)]\n",
    "\n",
    "# Set the values\n",
    "dataset.set_values(\"complexity_score\", complexity_scores)\n",
    "\n",
    "dataset.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbc5bc0",
   "metadata": {},
   "source": [
    "### Visual Similarity Through Embeddings\n",
    "\n",
    "> Embeddings are powerful: they convert images into high-dimensional vectors where visually similar images are close together. We'll use CLIP - a vision-language model trained on 400M image-text pairs - to embed our damage images.\n",
    ">\n",
    "> **Important note**: These embeddings capture the ENTIRE image - the car, the perspective, ALL the damages together. An image with 'dent + scratch' gets one embedding that represents that whole scene. This is different from patch embeddings, which we could compute per damage instance.\n",
    ">\n",
    "> Let's see what patterns emerge when we map these complex, multi-damage images into embedding space.\n",
    "\n",
    "- **`foz.load_zoo_model()`** loads a pre-trained model from the FiftyOne Model Zoo. In this case, it loads the CLIP ViT-B/32 model, which is commonly used for generating image embeddings.  \n",
    "  [Reference](https://github.com/voxel51/fiftyone/blob/develop/docs/source/getting_started/manufacturing/02_embeddings.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11624e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.zoo as foz\n",
    "\n",
    "model = foz.load_zoo_model(\"clip-vit-base32-torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6aff3b",
   "metadata": {},
   "source": [
    "- **`dataset.compute_embeddings()`** computes and stores embeddings for each image in the dataset using the specified model. The `embeddings_field` argument specifies the sample field where the resulting embedding vectors will be stored. Each image receives a single embedding vector representing the whole image.  \n",
    "  [Reference](https://github.com/voxel51/fiftyone/blob/develop/docs/source/getting_started/manufacturing/02_embeddings.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fed26e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2D visualization using UMAP\n",
    "import fiftyone.brain as fob\n",
    "\n",
    "results = fob.compute_visualization(\n",
    "    dataset,\n",
    "    embeddings=\"clip_embeddings\",\n",
    "    method=\"umap\",\n",
    "    brain_key=\"clip_viz\",\n",
    "    num_dims=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e15c6a",
   "metadata": {},
   "source": [
    "\n",
    "- **`fob.compute_visualization()`** performs dimensionality reduction (here, UMAP) on the stored embeddings to create a 2D representation for visualization. The `embeddings` argument specifies which field to use, `method=\"umap\"` selects the UMAP algorithm, and `brain_key` is used to store and retrieve the visualization results.  \n",
    "  [Reference](https://github.com/voxel51/fiftyone/blob/develop/docs/source/getting_started/manufacturing/02_embeddings.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b61371f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2D visualization using UMAP\n",
    "import fiftyone.brain as fob\n",
    "\n",
    "results = fob.compute_visualization(\n",
    "    dataset,\n",
    "    embeddings=\"clip_embeddings\",\n",
    "    method=\"umap\",\n",
    "    brain_key=\"clip_embeddings_viz\",\n",
    "    num_dims=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d08b3d2",
   "metadata": {},
   "source": [
    "#### Semantic Search Across Damage Scenarios \n",
    "\n",
    "We can search by complex queries that span multiple damages, perspectives, and contexts.\n",
    "\n",
    "Semantic search makes this possible - and it's incredibly valuable for building training sets, searching images with natural language, and quality control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7e0aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_img_index = fob.compute_similarity(\n",
    "    dataset,\n",
    "    model=\"clip-vit-base32-torch\",\n",
    "    brain_key=\"text_img_sim\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12748c03",
   "metadata": {},
   "source": [
    "### Representativeness: Finding \"Typical\" Damage Scenarios\n",
    "\n",
    "> Now let's ask: What does a 'typical' car damage image look like? And what are the edge cases?\n",
    "\n",
    "\n",
    "### Representativeness\n",
    "\n",
    "- **Definition:** Representativeness measures how well a sample typifies or summarizes the main patterns in your dataset. A highly representative sample is similar to many other samples—it sits near the center of a cluster in embedding space.\n",
    "\n",
    "- **Use case:** Useful for finding prototypical or \"easy\" examples, or for visualizing the main modes of your data. Representative samples are good for understanding the core structure of your dataset.\n",
    "\n",
    "- **How it's computed:** FiftyOne computes a scalar-valued `representativeness` field for each sample, also normalized to [0, 1], with 1 being the most representative. This is based on clustering—samples close to cluster centers are more representative.\n",
    "\n",
    "- **Example:** If you want to quickly understand the main types of data you have, or select examples that best summarize your dataset, you would look for high-representativeness samples.\n",
    "\n",
    "[Learn more about representativeness in the docs](https://docs.voxel51.com/brain.html#image-representativeness).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced19e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute representativeness\n",
    "print(\"Computing representativeness scores...\")\n",
    "\n",
    "fob.compute_representativeness(\n",
    "    dataset,\n",
    "    embeddings=\"clip_embeddings\",\n",
    "    representativeness_field=\"representativeness\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffedd606",
   "metadata": {},
   "source": [
    "## Cross-analyzing Complexity and Representativeness\n",
    "\n",
    "We want to identify samples that face **two challenges simultaneously**:\n",
    "\n",
    "1. **High complexity** - Many damages to detect (harder task)\n",
    "2. **Low representativeness** - Rare/unusual scenarios (less training data)\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "When a model struggles on complex images, it could be for two reasons:\n",
    "- **Inherent difficulty**: More objects = harder detection task\n",
    "- **Data scarcity**: Fewer training examples = less learned patterns\n",
    "\n",
    "The worst-case scenarios are images that suffer from **both** problems - they're hard to detect AND the model has seen few similar examples during training.\n",
    "\n",
    "### The Edge Case Score\n",
    "\n",
    "We compute an **edge case score** for each sample by:\n",
    "\n",
    "1. **Standardizing both metrics** (z-scores) to put them on the same scale\n",
    "2. **Combining them**: `edge_case_score = complexity_z - representativeness_z`\n",
    "\n",
    "**High scores** indicate samples that are:\n",
    "- More complex than average (high complexity_z)\n",
    "- Less representative than average (low representativeness, so subtracting it increases the score)\n",
    "\n",
    "### What We Learn\n",
    "\n",
    "- **Overall correlation**: Does complexity correlate with representativeness across the dataset? A negative correlation confirms that complex scenarios are indeed rare.\n",
    "\n",
    "- **Per-sample scores**: Which specific images are the \"double whammy\" cases that deserve special attention for data collection or model improvement?\n",
    "\n",
    "### Actionable Insights\n",
    "\n",
    "1. **Prioritize data collection**: Find more examples similar to high edge-case-score samples\n",
    "2. **Explain performance gaps**: Model struggles aren't just about task difficulty - it's also data availability\n",
    "3. **Strategic evaluation**: Separate \"hard because complex\" from \"hard because rare\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96dcf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "complexity = np.array(dataset.values(\"complexity_score\"))\n",
    "representativeness = np.array(dataset.values(\"representativeness\"))\n",
    "\n",
    "# Standardize both metrics (z-scores)\n",
    "complexity_z = (complexity - complexity.mean()) / complexity.std()\n",
    "representativeness_z = (representativeness - representativeness.mean()) / representativeness.std()\n",
    "\n",
    "# Compute \"edge case score\": high complexity + low representativeness\n",
    "# Flip representativeness so low values become high scores\n",
    "edge_case_score = complexity_z - representativeness_z\n",
    "\n",
    "dataset.set_values(\"edge_case_score\", edge_case_score.tolist())\n",
    "\n",
    "# Also store the overall correlation\n",
    "correlation = np.corrcoef(complexity, representativeness)[0,1]\n",
    "dataset.info[\"complexity_representativeness_correlation\"] = float(correlation)\n",
    "dataset.save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea40eda",
   "metadata": {},
   "source": [
    "### Scenario complexity\n",
    "\n",
    "**Not all data is created equal.** Some images are easy, some are hard - but for DIFFERENT reasons.\n",
    "\n",
    "By understanding WHAT makes data hard (unusual conditions vs. lots of stuff), you can make smarter decisions about training order, data collection priorities, and production routing.**\n",
    "\n",
    "It's the difference between \"some images are hard\" (vague) and \"images are hard because of X or Y\" (actionable).\n",
    "\n",
    "### **Two Independent Axes of Difficulty:**\n",
    "\n",
    "1. **Representativeness:** \"Is this a common scenario or a weird one?\"\n",
    "   - Think: normal lighting vs weird angle\n",
    "\n",
    "2. **Complexity:** \"How much stuff is in this image?\"\n",
    "   - Think: 1 damage vs 5 damages\n",
    "\n",
    "**An image can be hard for DIFFERENT reasons:**\n",
    "\n",
    "- **Hard because unusual:** Rare angle you haven't seen much (low rep, low complexity)\n",
    "\n",
    "- **Hard because complex:** Many damages to track (high complexity, high rep)\n",
    "\n",
    "- **Hard for BOTH:** Rare situation + many damages = nightmare scenario\n",
    "\n",
    "\n",
    "### **For Machine Learning:**\n",
    "\n",
    "Same principle:\n",
    "1. **Train on simple typical** → Model learns the basics\n",
    "\n",
    "2. **Add complex typical** → Model learns to handle multiple objects\n",
    "\n",
    "3. **Add simple edge** → Model learns robustness to conditions\n",
    "\n",
    "4. **Add complex edge** → Model becomes expert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578ccd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone import ViewField as F\n",
    "\n",
    "dataset.add_sample_field(\"scenario_complexity\", fo.StringField)\n",
    "\n",
    "rep_scenario_expr = (\n",
    "    ((F(\"representativeness\") > 0.7) & (F(\"complexity_score\") <= 3)).if_else(\n",
    "        \"simple_typical\",\n",
    "        ((F(\"representativeness\") > 0.6) & (F(\"complexity_score\") > 3)).if_else(\n",
    "            \"complex_typical\",\n",
    "            ((F(\"representativeness\") < 0.4) & (F(\"complexity_score\") <= 3)).if_else(\n",
    "                \"simple_edge\",\n",
    "                ((F(\"representativeness\") < 0.4) & (F(\"complexity_score\") > 3)).if_else(\n",
    "                    \"complex_edge\",\n",
    "                    \"other\"\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "dataset.set_field(\"scenario_complexity\", rep_scenario_expr)\n",
    "\n",
    "dataset.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abf1821",
   "metadata": {},
   "source": [
    "### Uniqueness: Finding Outlier Scenarios\n",
    "\n",
    "\n",
    "> Uniqueness tells us which images represent truly unusual scenarios - ones that don't fit into any common pattern.\n",
    "\n",
    "- **Definition:** Uniqueness measures how different a sample is from all other samples in the dataset. A higher uniqueness score means the sample is less similar to others—it's an outlier or rare example.\n",
    "\n",
    "- **Use case:** Useful for identifying and removing near-duplicate images, or for selecting the most unique samples to bootstrap model training. Unique samples help your model learn efficiently by exposing it to the full diversity of your data.\n",
    "\n",
    "- **How it's computed:** FiftyOne computes a scalar-valued `uniqueness` field for each sample, normalized to [0, 1], with 1 being the most unique sample in the dataset. This is based on the distance in embedding space to other samples—samples far from others are more unique.\n",
    "\n",
    "- **Example:** If you want to avoid bias or redundancy in your training data, you might filter for high-uniqueness samples to ensure diversity.\n",
    "  \n",
    "[Learn more about uniqueness in the docs](https://docs.voxel51.com/brain.html#image-uniqueness) and see the [tutorial](https://docs.voxel51.com/tutorials/uniqueness.html) for practical examples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e7a168",
   "metadata": {},
   "outputs": [],
   "source": [
    "fob.compute_uniqueness(\n",
    "    dataset,\n",
    "    embeddings=\"clip_embeddings\",\n",
    "    uniqueness_field=\"uniqueness\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a37682",
   "metadata": {},
   "source": [
    "**In summary:**  \n",
    "- **Uniqueness** finds outliers and rare examples.  \n",
    "- **Representativeness** finds prototypical, common examples.\n",
    "\n",
    "\n",
    "#### **When an image is an outlier (unique), WHY is it an outlier?**\n",
    "\n",
    "Are your outliers outliers because they're **genuinely rare situations** (complex) or just **photographed weirdly** (simple but unusual angle)?\n",
    "\n",
    "Computing the following score tells you whether to collect more complex scenarios vs. just augment for viewpoint/lighting variations.\n",
    "\n",
    "**Two Possibilities:**\n",
    "\n",
    "1. **Unique AND Complex:** Outlier because it has tons of damages (4+)\n",
    "   - Example: Car completely wrecked, 5 different damage types\n",
    "   - **Why outlier:** Legitimately rare scenario\n",
    "\n",
    "2. **Unique BUT Simple:** Outlier despite having few damages (1-2)\n",
    "   - Example: Single scratch, but photographed from underneath the car\n",
    "   - **Why outlier:** Weird angle/lighting, not the damage itself\n",
    "\n",
    "##### **Different causes = different solutions:**\n",
    "\n",
    "- **Unique + Complex:** Probably need more training data for multi-damage scenarios\n",
    "\n",
    "- **Unique + Simple:** Probably need better augmentation (weird angles, lighting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abb2ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone import ViewField as F\n",
    "\n",
    "dataset.add_sample_field(\"uniqueness_complexity_scenario\", fo.StringField)\n",
    "\n",
    "unq_scenario_expr = (\n",
    "    ((F(\"uniqueness\") > 0.7) & (F(\"complexity_score\") > 4)).if_else(\n",
    "        \"unique_and_complex\",\n",
    "        ((F(\"uniqueness\") > 0.7) & (F(\"complexity_score\") <= 2)).if_else(\n",
    "            \"unique_but_simple\",\n",
    "            \"other\"\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "dataset.set_field(\"uniqueness_complexity_scenario\", unq_scenario_expr)\n",
    "\n",
    "dataset.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae70862",
   "metadata": {},
   "source": [
    "### The Two-Dimensional Framework for Multi-Instance Data\n",
    "\n",
    "Let's bring this together with the representativeness-uniqueness framework, adapted for our multi-instance, multi-perspective challenge.\n",
    "\n",
    "```markdown\n",
    "        High Representativeness\n",
    "                │\n",
    "    \"Mainstream │   \"Suspicious\"\n",
    "    Scenarios\"  │   (investigate)\n",
    "   (Train here) │\n",
    "────────────────┼────────────────\n",
    "                │\n",
    "    \"Niche      │   \"True\n",
    "    Clusters\"   │   Outliers\"\n",
    "   (After base) │  (Review/Exclude)\n",
    "                │\n",
    "        Low Representativeness\n",
    "\n",
    "        Uniqueness →\n",
    "\n",
    "```\n",
    "\n",
    "For multi-instance detection, this framework helps us handle scenario diversity:\n",
    "\n",
    "• **Mainstream**: Train foundation model here - covers most insurance claims\n",
    "\n",
    "• **Niche**: Specialized handling - maybe fine-tune separately for rear-view damages\n",
    "\n",
    "• **Outliers**: Human review - too unusual for automated processing\n",
    "\n",
    "• **Suspicious**: Investigate - might be duplicates or errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5529d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone import ViewField as F\n",
    "\n",
    "dataset.add_sample_field(\"twod_scenario_analysis\", fo.StringField)\n",
    "\n",
    "twod_scenario_expr = (\n",
    "    ((F(\"representativeness\") > 0.6) & (F(\"uniqueness\") < 0.4)).if_else(\n",
    "        \"mainstream\",\n",
    "        ((F(\"representativeness\") < 0.4) & (F(\"uniqueness\") < 0.5)).if_else(\n",
    "            \"niche\",\n",
    "            ((F(\"representativeness\") < 0.4) & (F(\"uniqueness\") > 0.7)).if_else(\n",
    "                \"outlier\",\n",
    "                ((F(\"representativeness\") > 0.6) & (F(\"uniqueness\") > 0.7)).if_else(\n",
    "                    \"suspicious\",\n",
    "                    \"other\"\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "dataset.set_field(\"twod_scenario_analysis\", twod_scenario_expr)\n",
    "\n",
    "dataset.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af17a0e4",
   "metadata": {},
   "source": [
    "### Act 1 Wrap-Up\n",
    "\n",
    "\n",
    "**What Embeddings Revealed**:\n",
    "- The visual structure of our data - what looks similar, what's distinct\n",
    "\n",
    "- Which damage types will be easy to separate (glass shatter) vs confusing (scratch/crack)\n",
    "\n",
    "- How complexity and perspective affect clustering\n",
    "\n",
    "**What Representativeness Showed**:\n",
    "- What \"typical\" looks like in our dataset vs what's an edge case\n",
    "\n",
    "- How to stratify data by scenario difficulty\n",
    "\n",
    "- Where we might have training data gaps\n",
    "\n",
    "**What Uniqueness Identified**:\n",
    "\n",
    "- True outliers that don't fit any pattern\n",
    "\n",
    "- Which unusual samples are worth handling vs excluding\n",
    "\n",
    "- Different types of \"difficult\" (rare situation vs weird capture)\n",
    "\n",
    "**The Power of This Approach:**\n",
    "\n",
    "- We understand our data BEFORE spending time/money on training\n",
    "\n",
    "- We can discover where models will struggle and why\n",
    "\n",
    "- We can make strategic decisions about training order, data collection, and deployment\n",
    "\n",
    "This is data-centric AI for complex, real-world problems.\n",
    "\n",
    "\n",
    "### Let's go ahead an turn to the FiftyOne App to see all othes results on our dataset\n",
    "\n",
    "It will be helfpul to have The [Dashboard Plugin](https://docs.voxel51.com/plugins/plugins_ecosystem/dashboard.html) installed, which can be installed by running the following in your terminal:\n",
    "\n",
    "```bash\n",
    "fiftyone plugins download \\\n",
    "    https://github.com/voxel51/fiftyone-plugins \\\n",
    "    --plugin-names @voxel51/dashboard\n",
    "```\n",
    "## Act 2: Enrich\n",
    "\n",
    "Now let's add even richer context with Vision Language Models...\n",
    "\n",
    "### VLMs for Comprehensive Damage Reports\n",
    "\n",
    "Our current annotations tell us: 'This image has a dent at [x,y,w,h], a scratch at [x2,y2,w2,h2], and a crack at [x3,y3,w3,h3].'\n",
    "\n",
    "But for real-world applications - insurance claims, repair estimation, fleet management - we need more:\n",
    " \n",
    " - HOW do the damages relate? Is the scratch connected to the dent?\n",
    " \n",
    " - WHICH parts of the car are affected? Door? Bumper? Multiple panels?\n",
    " \n",
    " - What's the OVERALL severity? Minor cosmetic or structural concern?\n",
    " \n",
    " - Are there SECONDARY effects? Paint chipping? Rust? Deformation?\n",
    "\n",
    "Vision Language Models can generate these holistic damage assessments that consider ALL damages in context, plus the perspective and car condition.\n",
    "\n",
    "Note, we are using moondream3 here which is a gated model. Follow the instructions [here](https://github.com/harpreetsahota204/moondream3) to get access. But basically you just sign into Hugging Face, [request access to the model](https://huggingface.co/moondream/moondream3-preview) and you will have access instantaneously. Then run `hf auth login` in your terminal and pass your HF Token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0c5cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register and download the remotely-sourced zoo model\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "foz.register_zoo_model_source(\n",
    "    \"https://github.com/harpreetsahota204/moondream3\", \n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "foz.download_zoo_model(\n",
    "    \"https://github.com/harpreetsahota204/moondream3\",\n",
    "    model_name=\"moondream/moondream3-preview\"\n",
    ")\n",
    "\n",
    "moondream_model = foz.load_zoo_model(\"moondream/moondream3-preview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3b3ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "moondream_model.operation = \"query\"\n",
    "\n",
    "moondream_model.prompt = \"\"\"Complete a comprehensive damage report for this vehicle. \n",
    "Include:\n",
    "1. All visible damages and their locations on the vehicle\n",
    "2. How the damages relate to each other (if applicable)\n",
    "3. Overall severity assessment\n",
    "4. Any secondary effects (paint damage, deformation, etc.)\"\"\"\n",
    "\n",
    "dataset.apply_model(moondream_model, \"damage_report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342c9f47",
   "metadata": {},
   "source": [
    "We can use the Keyword Search Plugin and search the damage reports by keywords, install the plugin:\n",
    "\n",
    "```bash\n",
    "fiftyone plugins download fiftyone plugins download https://github.com/jacobmarks/keyword-search-plugin\n",
    "```\n",
    "\n",
    "We can also create a view programatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01e0640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find high-severity cases\n",
    "from fiftyone import ViewField as F\n",
    "high_severity = dataset.match(\n",
    "    F(\"damage_report\").contains(\"high severity\") |\n",
    "    F(\"damage_report\").contains(\"severe\") |\n",
    "    F(\"damage_report\").contains(\"safety-critical\")\n",
    ")\n",
    "dataset.save_view(\"high_severity_reports\", high_severity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b45b6c",
   "metadata": {},
   "source": [
    "You can also compute embeddings for the `damage_report` field we generated using the VLM and visualize those in the App:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070d6076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import fiftyone.brain as fob\n",
    "from transformers import AutoModel\n",
    "\n",
    "#set an environment variable so tokenizers doesn't yell at us,\n",
    "# note this related to the `transformers` and `tokenizers` libraries and not a FiftyOne specific environment variable\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "jina_embeddings_model = AutoModel.from_pretrained(\n",
    "    \"jinaai/jina-embeddings-v3\", \n",
    "    trust_remote_code=True,\n",
    "    device_map = \"auto\"\n",
    "    )\n",
    "\n",
    "for sample in dataset.iter_samples(autosave=True):\n",
    "    text_embeddings = jina_embeddings_model.encode(\n",
    "        sentences = [sample[\"damage_report\"]], # model expects a list of strings\n",
    "        task=\"separation\"\n",
    "        )\n",
    "    sample[\"text_embeddings\"] = text_embeddings.squeeze()\n",
    "\n",
    "\n",
    "results = fob.compute_visualization(\n",
    "    dataset,\n",
    "    embeddings=\"text_embeddings\",\n",
    "    method=\"umap\",\n",
    "    brain_key=\"text_embeddings\",\n",
    "    num_dims=2,\n",
    "    skip_failures=True,\n",
    "    create_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee82662",
   "metadata": {},
   "source": [
    "Before launching the app the [Caption Viewer plugin](https://docs.voxel51.com/plugins/plugins_ecosystem/caption_viewer.html) installed, which can be installed by running the following in your terminal:\n",
    "\n",
    "```bash\n",
    "# Install from GitHub\n",
    "fiftyone plugins download https://github.com/harpreetsahota204/caption-viewer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6c32a5",
   "metadata": {},
   "source": [
    "**Multi-perspective captures**: Cars photographed from various angles\n",
    "   - Front view, side view, rear view, close-ups, wide shots\n",
    "   - Same damage type looks different from different perspectives\n",
    "   - Adds complexity: model must recognize \"scratch\" from any angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60fb3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "moondream_model.operation = \"classify\"\n",
    "\n",
    "model.prompt = [\"Front view\", \"Side view\", \"Rear view\", \"Close-ups\", \"Wide angle shots\"]\n",
    "\n",
    "dataset.apply_model(moondream_model, \"camera_perspective\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de6a53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "moondream_model.operation = \"classify\"\n",
    "\n",
    "model.prompt = [\n",
    "    \"Front bumper damage\",\n",
    "    \"Rear bumper damage\",\n",
    "    \"Hood damage\",\n",
    "    \"Roof damage\",\n",
    "    \"Trunk damage\",\n",
    "    \"Door damage\",\n",
    "    \"Fender damage\",\n",
    "    \"Quarter panel damage\",\n",
    "    \"Windshield damage\",\n",
    "    \"Window damage\",\n",
    "    \"Headlight damage\",\n",
    "    \"Taillight damage\",\n",
    "    \"Mirror damage\",\n",
    "    \"Wheel damage\",\n",
    "    \"Tire damage\",\n",
    "    \"Grille damage\"\n",
    "]\n",
    "\n",
    "dataset.apply_model(moondream_model, \"damage_location\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ad942e",
   "metadata": {},
   "source": [
    "## Act 3: Build & Evaluate\n",
    "\n",
    "The goal here is to demonstrate that multi-instance detection evaluation requires going beyond simple metrics to understand per-instance performance, multi-damage confusion, and perspective invariance.\n",
    "\n",
    "\n",
    "In a longer-form version of this workshop, [I talk about using zero-shot models and how to fine-tune a model on this dataset](https://github.com/harpreetsahota204/car_dd_dataset_workshop/blob/main/03_model_evaluation.ipynb). We're going to skip those details in the interest of time, and take a model that I've already fine-tuned on this dataset and evaluate it's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd9ad8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/harpreetsahota/car-dd-segmentation-yolov11/resolve/main/best.pt -O yolov11-seg-cardd.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e706ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.zoo as foz\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model_path = \"yolov11-seg-cardd.pt\"\n",
    "\n",
    "yolo_model = YOLO(model_path)\n",
    "\n",
    "dataset.apply_model(yolo_model, label_field=\"yolo_predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3364e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick stats on what was detected\n",
    "total_gt = dataset.count(\"detections.detections\")\n",
    "\n",
    "total_pred = dataset.count(\"yolo_predictions.detections\")\n",
    "\n",
    "print(f\"\\nGround truth instances: {total_gt}\")\n",
    "\n",
    "print(f\"Predicted instances: {total_pred}\")\n",
    "\n",
    "print(f\"Detection ratio: {total_pred/total_gt:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27ad808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images where we detected fewer instances than GT\n",
    "under_detected = dataset.match(\n",
    "    F(\"yolo_predictions.detections\").length() < \n",
    "    F(\"detections.detections\").length()\n",
    ")\n",
    "\n",
    "dataset.save_view(\"under_detected\", under_detected)\n",
    "\n",
    "print(f\"Under-detected (missed some damages): {len(under_detected)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9d485a",
   "metadata": {},
   "outputs": [],
   "source": [
    "over_detected = dataset.match(\n",
    "    F(\"yolo_predictions.detections\").length() > \n",
    "    F(\"detections.detections\").length()\n",
    ")\n",
    "\n",
    "dataset.save_view(\"over_detected\", over_detected)\n",
    "\n",
    "print(f\"Over-detected: {len(over_detected)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3785ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perfect instance count match\n",
    "perfect_count = dataset.match(\n",
    "    F(\"yolo_predictions.detections\").length() == \n",
    "    F(\"detections.detections\").length()\n",
    ")\n",
    "\n",
    "dataset.save_view(\"perfect_count\", perfect_count)\n",
    "\n",
    "print(f\"Perfect instance count: {len(perfect_count)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935e2da6",
   "metadata": {},
   "source": [
    "Evaluating multi-instance detection is complex. We need to:\n",
    " - Match predicted instances to GT instances (Hungarian matching)\n",
    " - Compute precision/recall at the instance level\n",
    " - Account for images with varying numbers of damages\n",
    " - Understand per-class performance when damages co-occur\n",
    "\n",
    "FiftyOne's evaluation framework handles all of this with COCO-style evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846575f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dataset.evaluate_detections(\n",
    "    \"yolo_predictions\",\n",
    "    gt_field=\"detections\",\n",
    "    eval_key=\"eval\",\n",
    "    compute_mAP=True,\n",
    "    method=\"coco\",\n",
    "    use_boxes=True  # Can also use use_masks=True for mask IoU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c94cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall metrics\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"OVERALL MULTI-INSTANCE DETECTION PERFORMANCE\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"mAP @ IoU=0.50:0.95: {results.mAP():.3f}\")\n",
    "print(f\"mAP @ IoU=0.50:      {results.mAP(iou=0.5):.3f}\")\n",
    "print(f\"mAP @ IoU=0.75:      {results.mAP(iou=0.75):.3f}\")\n",
    "\n",
    "# Per-class breakdown\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"PER-CLASS BREAKDOWN (Remember: multi-damage images affect all classes)\")\n",
    "print(f\"{'='*70}\")\n",
    "results.print_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1fbf9b",
   "metadata": {},
   "source": [
    "You should also install the [Model Evaluation Panel](https://docs.voxel51.com/user_guide/app.html#app-model-evaluation-panel), which can be installed by running the following in your terminal:\n",
    "\n",
    "```bash\n",
    "fiftyone plugins download \\\n",
    "    https://github.com/voxel51/fiftyone-plugins \\\n",
    "    --plugin-names @voxel51/evaluation\n",
    "```\n",
    "\n",
    "This will let us do some more fine grained analysis in the FiftyOne app.\n",
    "\n",
    "We can also create some views that we can inspect in the App.\n",
    "\n",
    "> \"In multi-damage scenarios (2+ damages), what is the model confidently predicting that's actually wrong?\"\n",
    "\n",
    "**Step 1** (`.match()`): \"Give me multi-damage images that have at least one false positive\"\n",
    "\n",
    "**Step 2** (`.filter_labels()`): \"Now hide everything except the high-confidence FPs\"\n",
    "\n",
    "You see images with multiple damages, but only the false positive predictions are visible. This makes it easier to examine what the model got wrong without distraction from correct predictions.\n",
    "\n",
    "\n",
    "- `.match()` with `.contains(\"fp\")` = **select which images to show**\n",
    "\n",
    "- `.filter_labels()` with `F(\"eval\") == \"fp\"` = **select which detections to show within those images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff6b356",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === FALSE POSITIVES (predicted damage that's not there) ===\n",
    "# High-confidence FPs in multi-damage scenarios\n",
    "multi_damage_fps = dataset.match(\n",
    "    (F(\"detections.detections\").length() > 1) &\n",
    "    (F(\"yolo_predictions.detections.eval\").contains(\"fp\"))  # Step 1: Get images with FPs\n",
    ").filter_labels(\n",
    "    \"yolo_predictions\",\n",
    "    (F(\"eval\") == \"fp\") & (F(\"confidence\") > 0.7)  # Step 2: Show only those FPs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72942bd",
   "metadata": {},
   "source": [
    "> \"In complex scenarios with many damages, which specific damages did the model miss?\"\n",
    "\n",
    "**Step 1** (`.match()`): Get images with 3+ damages (complex scenarios)\n",
    "\n",
    "**Step 2** (`.filter_labels()`): Show only the ground truth detections that were missed (false negatives)\n",
    "\n",
    "**Result:** You see complex multi-damage images, but only the damages the model **failed to detect** are visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bf3441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FALSE NEGATIVES (missed damage) ===\n",
    "\n",
    "# Missed damages in complex scenarios\n",
    "missed_in_complex = dataset.match(\n",
    "    F(\"detections.detections\").length() > 2\n",
    ").filter_labels(\n",
    "    \"detections\",\n",
    "    F(\"eval\") == \"fn\"\n",
    ")\n",
    "dataset.save_view(\"missed_in_complex\", missed_in_complex)\n",
    "\n",
    "print(f\"Missed damages in complex scenes: {missed_in_complex.count('detections.detections')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1056b5",
   "metadata": {},
   "source": [
    "We can now open the app and look more closely at the model evaluation panel and do some scenario analysis.\n",
    "\n",
    "## Act 4: Deployment Strategy\n",
    "\n",
    "The goal here is to show how to validate label quality for multi-instance annotations and build risk-stratified deployment strategies that account for scenario complexity.\n",
    "\n",
    "Multi-instance annotations are prone to errors:\n",
    "\n",
    " - **Missed instances**: Annotator overlooked a damage\n",
    " - **Wrong labels**: Scratch vs crack confusion\n",
    " - **Poor localization**: Bounding box doesn't capture full damage\n",
    " - **Merged instances**: Two damages annotated as one\n",
    "\n",
    "[FiftyOne's mistakenness](https://docs.voxel51.com/brain.html#brain-label-mistakes) helps find these issues by comparing confident predictions against ground truth.\n",
    "\n",
    "This algorithm finds potential annotation errors by checking when confident model predictions disagree with your ground truth labels.\n",
    "\n",
    "The core idea is simple: if your model is really confident about a prediction but your ground truth says something different, there's probably a labeling mistake. The algorithm calculates a \"mistakenness score\" that's high when the model is confident and wrong, suggesting the ground truth might be incorrect rather than the model. It works for both classification (wrong class label) and localization (wrong bounding box position).\n",
    "\n",
    "This helps you clean up datasets by automatically flagging suspicious annotations for human review.\n",
    "\n",
    "You can read more detail about exactly this works by looking under the \"Mistakenness\" section of this [notebook](https://github.com/harpreetsahota204/car_dd_dataset_workshop/blob/main/03_model_evaluation.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b297b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fob.compute_mistakenness(\n",
    "    dataset,\n",
    "    \"yolo_predictions\",\n",
    "    label_field=\"detections\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a665d13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze mistakenness patterns\n",
    "from fiftyone import ViewField as F\n",
    "\n",
    "# High mistakenness samples\n",
    "high_mistake_samples = dataset.match(F(\"mistakenness\") > 0.7)\n",
    "print(f\"\\nHigh mistakenness samples: {len(high_mistake_samples)}\")\n",
    "print(f\"Avg complexity: {high_mistake_samples.mean('complexity_score'):.2f}\")\n",
    "\n",
    "# High mistakenness instances\n",
    "high_mistake_instances = dataset.filter_labels(\n",
    "    \"detections\",\n",
    "    F(\"mistakenness\") > 0.8\n",
    ")\n",
    "print(f\"High mistakenness instances: {high_mistake_instances.count('detections.detections')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98595121",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nMistakenness by damage type:\")\n",
    "for label in dataset.distinct(\"detections.detections.label\"):\n",
    "    view = dataset.filter_labels(\n",
    "        \"detections\",\n",
    "        (F(\"label\") == label) & (F(\"mistakenness\") > 0.7)\n",
    "    )\n",
    "    count = view.count(\"detections.detections\")\n",
    "    total = len(dataset.filter_labels(\"detections\", F(\"label\") == label))\n",
    "    pct = (count / total * 100) if total > 0 else 0\n",
    "    print(f\"  {label:15s}: {count:3d} / {total:4d} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfba68a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible missing annotations (model found damage, but no GT)\n",
    "possible_missing = dataset.filter_labels(\n",
    "    \"yolo_predictions\",\n",
    "    F(\"possible_missing\") == True\n",
    ")\n",
    "print(f\"\\nPossible missing annotations: {possible_missing.count('yolo_predictions.detections')}\")\n",
    "\n",
    "# Possible spurious annotations (GT exists but model never found it)\n",
    "possible_spurious = dataset.filter_labels(\n",
    "    \"detections\",\n",
    "    F(\"possible_spurious\") == True\n",
    ")\n",
    "print(f\"Possible spurious annotations: {possible_spurious.count('detections.detections')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a2981a",
   "metadata": {},
   "source": [
    "### Act 4: Multi-Instance Deployment Strategy\n",
    "\n",
    "Let's design a production deployment strategy that accounts for:\n",
    " - Scenario complexity (# of damages)\n",
    " - Representativeness (typical vs edge case)\n",
    " - Model confidence\n",
    " - Damage type (cracks need special handling)\n",
    "\n",
    "We'll create risk tiers for automotive insurance workflows.\n",
    "\n",
    "##### **Tier 1 - Auto-Approve:**\n",
    "\n",
    "Images where:\n",
    "- **Scenario is manageable:** Typical conditions OR few damages (≤2)\n",
    "- **Model is confident:** All predictions >70% confidence\n",
    "- **Detection is complete:** Found the right number of damages\n",
    "\n",
    "**Translation:** \"Model handled this well and we trust it\" → Process automatically without human review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e6ab28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone import ViewField as F\n",
    "\n",
    "# === TIER 1: AUTO-APPROVE (High confidence, simple/typical scenarios) ===\n",
    "\n",
    "tier1_auto = dataset.match(\n",
    "    # Typical OR simple scenarios\n",
    "    ((F(\"representativeness\") > 0.6) | (F(\"complexity_score\") <= 2)) &\n",
    "    # High model confidence (if has predictions)\n",
    "    ((F(\"yolo_predictions.detections.confidence\").length() == 0) |\n",
    "     (F(\"yolo_predictions.detections.confidence\").min() > 0.7)) &\n",
    "    # Complete detection (predicted count matches GT count)\n",
    "    (F(\"yolo_predictions.detections\").length() == \n",
    "     F(\"detections.detections\").length())\n",
    ")\n",
    "dataset.save_view(\"tier1_auto_approve\", tier1_auto)\n",
    "\n",
    "auto_count = len(tier1_auto)\n",
    "auto_instances = tier1_auto.count(\"detections.detections\")\n",
    "\n",
    "print(f\"TIER 1 - AUTO-APPROVE:\")\n",
    "print(f\"  Criteria: Typical/simple + complete detection + high confidence\")\n",
    "print(f\"  Samples: {auto_count} ({auto_count/len(dataset)*100:.1f}%)\")\n",
    "print(f\"  Instances: {auto_instances}\")\n",
    "print(f\"  Avg complexity: {tier1_auto.mean('complexity_score'):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4ddbe6",
   "metadata": {},
   "source": [
    "##### **Tier 2 - Expert Review:**\n",
    "\n",
    "Images where:\n",
    "- **Scenario is challenging:** Many damages (3+) OR unusual conditions OR model missed/added damages\n",
    "- **Not confident enough for auto-approval**\n",
    "\n",
    "**Translation:** \"Something is tricky here\" → Route to human expert for review before processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcecae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TIER 2: EXPERT REVIEW (Complex OR edge cases OR partial detection) ===\n",
    "\n",
    "tier2_expert = dataset.match(\n",
    "    # Complex scenarios\n",
    "    ((F(\"complexity_score\") > 3) |\n",
    "    # Edge cases\n",
    "     (F(\"representativeness\") < 0.4) |\n",
    "    # Partial detection\n",
    "     (F(\"yolo_predictions.detections\").length() != \n",
    "      F(\"detections.detections\").length())) &\n",
    "    # Not already in tier 1\n",
    "    (~F(\"id\").is_in(tier1_auto.values(\"id\")))\n",
    ")\n",
    "dataset.save_view(\"tier2_expert_review\", tier2_expert)\n",
    "\n",
    "expert_count = len(tier2_expert)\n",
    "expert_instances = tier2_expert.count(\"detections.detections\")\n",
    "\n",
    "print(f\"\\nTIER 2 - EXPERT REVIEW:\")\n",
    "print(f\"  Criteria: Complex OR edge OR partial detection\")\n",
    "print(f\"  Samples: {expert_count} ({expert_count/len(dataset)*100:.1f}%)\")\n",
    "print(f\"  Instances: {expert_instances}\")\n",
    "print(f\"  Avg complexity: {tier2_expert.mean('complexity_score'):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7591d47",
   "metadata": {},
   "source": [
    "#### **Tier 3 - Senior Adjuster:**\n",
    "\n",
    "Images where:\n",
    "- **High risk of errors:** Contains cracks (ambiguous damage type) OR very complex (4+ damages) OR labels look suspicious\n",
    "- **Requires expertise:** Beyond standard review\n",
    "\n",
    "**Translation:** \"This is ambiguous/complex and could be wrong\" → Route to senior expert with specialized damage assessment skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50cb741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TIER 3: SENIOR ADJUSTER (Cracks OR very high complexity OR high mistakenness) ===\n",
    "\n",
    "tier3_senior = dataset.match(\n",
    "    # Contains cracks (high confusion damage type)\n",
    "    (F(\"detections.detections.label\").contains(\"crack\") |\n",
    "    # Very high complexity\n",
    "     (F(\"complexity_score\") > 4) |\n",
    "    # High mistakenness\n",
    "     (F(\"mistakenness\") > 0.7)) &\n",
    "    # Not in tier 1\n",
    "    (~F(\"id\").is_in(tier1_auto.values(\"id\")))\n",
    ")\n",
    "dataset.save_view(\"tier3_senior_review\", tier3_senior)\n",
    "\n",
    "senior_count = len(tier3_senior)\n",
    "senior_instances = tier3_senior.count(\"detections.detections\")\n",
    "\n",
    "print(f\"\\nTIER 3 - SENIOR ADJUSTER:\")\n",
    "print(f\"  Criteria: Contains cracks OR very complex OR suspicious labels\")\n",
    "print(f\"  Samples: {senior_count} ({senior_count/len(dataset)*100:.1f}%)\")\n",
    "print(f\"  Instances: {senior_instances}\")\n",
    "print(f\"  Avg complexity: {tier3_senior.mean('complexity_score'):.2f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e9f78a",
   "metadata": {},
   "source": [
    "#### **Tier 4 - Data Collection Gaps:**\n",
    "\n",
    "Images where:\n",
    "- **Underrepresented scenarios:** Unusual crack cases OR unique complex situations (outliers with many damages)\n",
    "- **Not enough training examples**\n",
    "\n",
    "**Translation:** \"Model struggles here because we lack training data\" → Prioritize collecting more similar examples to improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419f6e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TIER 4: DATA COLLECTION GAPS (Need more training data) ===\n",
    "\n",
    "tier4_gaps = dataset.match(\n",
    "    # Cracks in edge cases\n",
    "    ((F(\"detections.detections.label\").contains(\"crack\")) &\n",
    "     (F(\"representativeness\") < 0.5)) |\n",
    "    # High uniqueness complex scenarios\n",
    "    ((F(\"uniqueness\") > 0.7) & (F(\"complexity_score\") > 3))\n",
    ")\n",
    "dataset.save_view(\"tier4_data_gaps\", tier4_gaps)\n",
    "\n",
    "gaps_count = len(tier4_gaps)\n",
    "\n",
    "print(f\"\\nTIER 4 - DATA COLLECTION GAPS:\")\n",
    "print(f\"  Criteria: Edge-case cracks OR unique complex scenarios\")\n",
    "print(f\"  Samples: {gaps_count}\")\n",
    "print(f\"  Avg complexity: {tier4_gaps.mean('complexity_score'):.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0d6039",
   "metadata": {},
   "source": [
    "## Workshop Wrap-Up: Key Takeaways \n",
    "\n",
    "In this workshop, we covered a complete workflow for multi-instance detection problems...\n",
    "\n",
    "**1. Explore Before Training**\n",
    "\n",
    "- Quantify dataset complexity (objects per sample, co-occurrence patterns)\n",
    "\n",
    "- Use embeddings to reveal visual structure\n",
    "\n",
    "- Identify typical vs edge cases with representativeness metrics\n",
    "\n",
    "**2. Evaluate at Instance Level**\n",
    "\n",
    "- Per-object metrics, not just per-image accuracy\n",
    "\n",
    "- Understand partial detection (finding some but not all objects)\n",
    "\n",
    "- Stratify performance by scenario complexity\n",
    "\n",
    "**3. Add Semantic Context**\n",
    "\n",
    "- VLMs capture relationships between objects\n",
    "\n",
    "- Generate searchable descriptions from visual data\n",
    "\n",
    "- Bridge gaps between bounding boxes and business needs\n",
    "\n",
    "**4. Validate Quality Systematically**\n",
    "\n",
    "- Use model confidence to find annotation errors\n",
    "\n",
    "- Focus review on ambiguous object types\n",
    "\n",
    "- Distinguish model failures from labeling mistakes\n",
    "\n",
    "**5. Deploy with Risk Awareness**\n",
    "\n",
    "- Auto-process high-confidence typical cases\n",
    "\n",
    "- Route complex scenarios to human experts\n",
    "\n",
    "- Build systems that degrade gracefully\n",
    "\n",
    "### Key Principles\n",
    "\n",
    "**Multi-instance problems require multi-dimensional thinking:**\n",
    "\n",
    " - Complexity is measurable (count objects, diversity, unusualness)\n",
    "\n",
    " - Partial success is normal (models find some objects, miss others)\n",
    "\n",
    " - Context matters (relationships between objects)\n",
    "\n",
    " - Stratification is essential (simple ≠ complex)\n",
    "\n",
    " - Instance-level evaluation required\n",
    "\n",
    " **This methodology generalizes to any multi-object detection problem.**\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Join one of our upcoming virtual events: https://voxel51.com/events\n",
    "\n",
    "- Join our community Discord where you can ask any questions you may have: https://discord.com/invite/fiftyone-community\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
